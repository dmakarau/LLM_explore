{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPDIDgUIGEimF9oq//O2Yyc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dmakarau/LLM_explore/blob/main/part1_text2num_preparingText4Tokens.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__nZlbSUIxpQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import requests\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get a text from Web"
      ],
      "metadata": {
        "id": "epipKuysN2p9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get the text from the internet\n",
        "book = requests.get('https://gutenberg.org/cache/epub/35/pg35.txt')\n",
        "\n",
        "# get text from the response\n",
        "text = book.text\n"
      ],
      "metadata": {
        "id": "i6yqr3ilI_Sx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# replace character strings with space\n",
        "stringsToReplace = [\n",
        "                 '\\r\\n\\r\\nâ\\x80\\x9c', # new paragraph\n",
        "                 'â\\x80\\x9c',         # open quote\n",
        "                 'â\\x80\\x9d',         # close quote\n",
        "                 '\\r\\n',              # new line\n",
        "                 'â\\x80\\x94',         # hyphen\n",
        "                 'â\\x80\\x99',         # single apostrophe\n",
        "                 'â\\x80\\x98',         # single quote\n",
        "                 '_',                 # underscore, used for stressing\n",
        "                 ]"
      ],
      "metadata": {
        "id": "1jN1vK7QKWl1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use regex to replace those strings with space\n",
        "for string in stringsToReplace:\n",
        "  regex = re.compile(r'%s'%string)\n",
        "  text = regex.sub(' ', text)\n",
        "\n",
        "# remove non ASCII chars\n",
        "text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
        "\n",
        "# remove numbers\n",
        "text = re.sub(r'\\d+', ' ', text)\n",
        "\n",
        "# make everything lowercase\n",
        "text = text.lower()\n",
        "\n",
        "text[:2000]"
      ],
      "metadata": {
        "id": "EDBes2_kKrA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parsing a text to words"
      ],
      "metadata": {
        "id": "Nn_oykxaN6pK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split text by punctuation\n",
        "import string\n",
        "puncts4re = rf'[{string.punctuation}\\s]+'\n",
        "\n",
        "words = re.split(puncts4re, text)\n",
        "words = [item.strip() for item in words if item.strip()]\n",
        "\n",
        "\n",
        "# remove single-character words\n",
        "words = [item for item in words if len(item) > 1]\n"
      ],
      "metadata": {
        "id": "JplL2dFIN-n2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a vocac - unique words\n",
        "vocab = sorted(set(words))\n",
        "# convencience variables\n",
        "words_length = len(words)\n",
        "vocab_length = len(vocab)\n",
        "print(f'Number of words: {words_length}')\n",
        "print(f'Number of unique words: {vocab_length}')"
      ],
      "metadata": {
        "id": "ZGGFk0T2Pu1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create token dictionaries and encoder/decoder  functions"
      ],
      "metadata": {
        "id": "srC5GBbhTap9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word2idx = { word :index  for index, word in enumerate(vocab)}\n",
        "idx2word = { index :word  for index, word in enumerate(vocab)}\n",
        "\n",
        "for i in list(word2idx.items())[:10000:87]:\n",
        "  print(i)\n"
      ],
      "metadata": {
        "id": "KnY03GHNTfME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#encode\n",
        "def encode(word_list, encode_dict):\n",
        "  # init a vector with numberical indices\n",
        "  idxs = np.zeros(len(word_list), dtype=int)\n",
        "\n",
        "  # loop through the words and find their indices in vocab\n",
        "  for index, word in enumerate(word_list):\n",
        "    idxs[index] = encode_dict[word]\n",
        "\n",
        "  return idxs\n",
        "\n",
        "# decode\n",
        "def decode(idx, decode_dict):\n",
        "  return ' '.join([ decode_dict[i] for i in idx ])"
      ],
      "metadata": {
        "id": "qa90HC--UeB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# testing the encode\n",
        "print(encode(['the', 'time', 'machine'], word2idx))\n",
        "\n",
        "# testing the decode\n",
        "print(decode([4338, 4405, 2596], idx2word))"
      ],
      "metadata": {
        "id": "56rcgYuDrpEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test encode then decode\n",
        "\n",
        "# random start location\n",
        "startidx = np.random.choice(words_length - 10)\n",
        "\n",
        "# sequential word indices\n",
        "idxs = np.arange(startidx, startidx + 10)\n",
        "\n",
        "print(\"Word indices:\")\n",
        "print(idxs), print('')\n",
        "\n",
        "print(\"The words\")\n",
        "wordseq = [ words[i] for i in idxs ]\n",
        "print(wordseq), print('')\n",
        "\n",
        "print('Token indices:')\n",
        "tokenseq = encode(wordseq, word2idx)\n",
        "print(tokenseq), print('')\n",
        "\n",
        "print('Decoded sentence:')\n",
        "print(decode(tokenseq, idx2word))\n"
      ],
      "metadata": {
        "id": "uHbf78UktqTk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}