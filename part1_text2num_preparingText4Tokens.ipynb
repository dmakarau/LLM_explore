{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNzNtRy/U9W1+UASrqQ19dC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dmakarau/LLM_explore/blob/main/part1_text2num_preparingText4Tokens.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__nZlbSUIxpQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import requests\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get a text from Web"
      ],
      "metadata": {
        "id": "epipKuysN2p9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get the text from the internet\n",
        "book = requests.get('https://gutenberg.org/cache/epub/35/pg35.txt')\n",
        "\n",
        "# get text from the response\n",
        "text = book.text\n"
      ],
      "metadata": {
        "id": "i6yqr3ilI_Sx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# replace character strings with space\n",
        "stringsToReplace = [\n",
        "                 '\\r\\n\\r\\nâ\\x80\\x9c', # new paragraph\n",
        "                 'â\\x80\\x9c',         # open quote\n",
        "                 'â\\x80\\x9d',         # close quote\n",
        "                 '\\r\\n',              # new line\n",
        "                 'â\\x80\\x94',         # hyphen\n",
        "                 'â\\x80\\x99',         # single apostrophe\n",
        "                 'â\\x80\\x98',         # single quote\n",
        "                 '_',                 # underscore, used for stressing\n",
        "                 ]"
      ],
      "metadata": {
        "id": "1jN1vK7QKWl1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use regex to replace those strings with space\n",
        "for string in stringsToReplace:\n",
        "  regex = re.compile(r'%s'%string)\n",
        "  text = regex.sub(' ', text)\n",
        "\n",
        "# remove non ASCII chars\n",
        "text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
        "\n",
        "# remove numbers\n",
        "text = re.sub(r'\\d+', ' ', text)\n",
        "\n",
        "# make everything lowercase\n",
        "text = text.lower()\n",
        "\n",
        "text[:2000]"
      ],
      "metadata": {
        "id": "EDBes2_kKrA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parsing a text to words"
      ],
      "metadata": {
        "id": "Nn_oykxaN6pK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split text by punctuation\n",
        "import string\n",
        "puncts4re = rf'[{string.punctuation}\\s]+'\n",
        "\n",
        "words = re.split(puncts4re, text)\n",
        "words = [item.strip() for item in words if item.strip()]\n",
        "\n",
        "\n",
        "# remove single-character words\n",
        "words = [item for item in words if len(item) > 1]\n"
      ],
      "metadata": {
        "id": "JplL2dFIN-n2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a vocac - unique words\n",
        "vocab = sorted(set(words))\n",
        "# convencience variables\n",
        "words_length = len(words)\n",
        "vocab_length = len(vocab)\n",
        "print(f'Number of words: {words_length}')\n",
        "print(f'Number of unique words: {vocab_length}')"
      ],
      "metadata": {
        "id": "ZGGFk0T2Pu1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create token dictionaries and encoder/decoder  functions"
      ],
      "metadata": {
        "id": "srC5GBbhTap9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word2idx = { word :index  for index, word in enumerate(vocab)}\n",
        "idx2word = { index :word  for index, word in enumerate(vocab)}\n",
        "\n",
        "for i in list(word2idx.items())[:10000:87]:\n",
        "  print(i)\n"
      ],
      "metadata": {
        "id": "KnY03GHNTfME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#encode\n",
        "def encode(word_list, encode_dict):\n",
        "  # init a vector with numberical indices\n",
        "  idxs = np.zeros(len(word_list), dtype=int)\n",
        "\n",
        "  # loop through the words and find their indices in vocab\n",
        "  for index, word in enumerate(word_list):\n",
        "    idxs[index] = encode_dict[word]\n",
        "\n",
        "  return idxs\n",
        "\n",
        "# decode\n",
        "def decode(idx, decode_dict):\n",
        "  return ' '.join([ decode_dict[i] for i in idx ])\n"
      ],
      "metadata": {
        "id": "qa90HC--UeB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# testing the encode\n",
        "print(encode(['the', 'time', 'machine'], word2idx))\n",
        "\n",
        "# testing the decode\n",
        "print(decode([4338, 4405, 2596], idx2word))"
      ],
      "metadata": {
        "id": "56rcgYuDrpEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test encode then decode\n",
        "\n",
        "# random start location\n",
        "startidx = np.random.choice(words_length - 10)\n",
        "\n",
        "# sequential word indices\n",
        "idxs = np.arange(startidx, startidx + 10)\n",
        "\n",
        "print(\"Word indices:\")\n",
        "print(idxs), print('')\n",
        "\n",
        "print(\"The words\")\n",
        "wordseq = [ words[i] for i in idxs ]\n",
        "print(wordseq), print('')\n",
        "\n",
        "print('Token indices:')\n",
        "tokenseq = encode(wordseq, word2idx)\n",
        "print(tokenseq), print('')\n",
        "\n",
        "print('Decoded sentence:')\n",
        "print(decode(tokenseq, idx2word))\n"
      ],
      "metadata": {
        "id": "uHbf78UktqTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 2 A random walk through the time machine"
      ],
      "metadata": {
        "id": "-t5yH748zoFl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A Brief Aside of Brownian Noise\n",
        "import matplotlib.pyplot as plt\n",
        "brownNoise = np.cumsum(np.random.choice([-1, 1], 3000))\n",
        "\n",
        "plt.figure(figsize=(10,3))\n",
        "plt.plot(brownNoise, 'k')\n",
        "plt.gca().set(xlim= [0, len(brownNoise)], xlabel = '\"Time\" ?', ylabel = 'Signal amplitude')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zv0hCnNg0Af1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Brownian noise\n",
        "brownNoise = np.cumsum(np.random.choice([-1, 1], 30))\n",
        "print(brownNoise)\n",
        "\n",
        "BrownianRandomTokens = brownNoise + np.random.choice(vocab_length, 1)\n",
        "\n",
        "# test with random token indices\n",
        "print(f'Token indices: {BrownianRandomTokens}')\n",
        "print(f'Decoded sentence: {decode(BrownianRandomTokens, idx2word)}')\n"
      ],
      "metadata": {
        "id": "mYS2tO8u3RR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 3. Distribution of Words Length"
      ],
      "metadata": {
        "id": "UmmxyLDc7Ou2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "wordsCount = []\n",
        "for word in words:\n",
        "  wordsCount.append(len(word))\n",
        "\n",
        "# Create a list of indices for the words in the order they appear in the text\n",
        "word_positions = list(range(len(words)))\n",
        "\n",
        "\n",
        "# Create the scatter plot\n",
        "plt.figure(figsize=(15, 6)) # Increased width to 15\n",
        "plt.scatter(word_positions, wordsCount, s=5) # Use s to adjust marker size\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel('Word Position in Text')\n",
        "plt.ylabel('Word Length')\n",
        "plt.title('Word Length by Position in Text')\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n",
        "\n",
        "plt.hist(wordsCount,rwidth=0.8)\n",
        "plt.xlabel('Word Length')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Histogram of Character Count Frequences')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OWQKgGqo7V-M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}